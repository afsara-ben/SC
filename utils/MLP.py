import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import matplotlib.pyplot as plt
import numpy as np
import pickle


# Define the two-layer MLP model
class MLP(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(MLP, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        out = self.fc1(x)
        out = self.relu(out)
        out = self.fc2(out)
        out = self.relu(out)  # Apply ReLU activation in the output layer
        return out


# # Define the dictionary
# data_dict = {2.11: 50, 2.82: 80, 3.71: 64, 2.24: 52, 2.3: 80, 2.56: 60, 2.75: 68, 1.28: 36, 2.43: 76, 2.05: 80,
#              1.79: 62, 2.62: 70, 3.33: 82, 2.37: 70, 2.94: 70, 3.2: 88, 1.41: 38, 1.54: 38, 3.84: 74, 1.6: 54, 2.18: 70,
#              1.73: 44, 1.86: 48, 0.9: 26, 2.69: 62, 1.98: 52, 3.14: 82, 1.92: 68, 3.46: 78, 1.34: 54, 2.5: 82, 1.09: 10,
#              4.16: 72, 3.26: 66, 3.39: 90, 3.9: 86, 3.97: 74, 2.88: 84, 1.02: 44, 3.01: 70, 1.15: 10, 1.66: 76,
#              0.96: 18, 3.65: 78, 3.52: 82, 4.22: 88, 1.47: 44, 4.42: 78, 3.78: 84, 4.1: 86, 3.58: 76, 3.07: 86,
#              4.74: 78, 6.4: 64, 1.22: 24, 4.35: 68, 4.48: 78, 4.03: 88, 5.18: 88, 5.44: 86, 5.06: 60, 4.29: 86,
#              3.73: 84, 3.7: 42, 3.13: 70, 3.43: 40, 3.19: 54, 3.03: 54, 2.8: 88, 2.76: 64, 3.25: 64, 3.89: 66, 3.57: 60,
#              2.96: 36, 3.69: 88, 2.78: 36, 3.09: 40, 2.93: 62, 3.17: 44, 2.79: 74, 3.35: 78, 3.94: 42, 3.79: 44,
#              3.41: 68, 3.6: 36, 2.9: 88, 3.18: 38, 3.08: 36, 3.02: 42, 3.21: 64, 2.85: 90, 3.27: 88, 3.88: 68, 3.63: 46,
#              3.74: 38, 3.62: 86, 2.77: 90, 3.34: 74, 2.91: 82, 3.1: 52}
#

# k_means_data_dict = {2.62: 395, 2.82: 365, 3.71: 569, 3.52: 614, 2.24: 572, 2.3: 269, 2.56: 635, 2.75: 566, 1.28: 293, 2.05: 509, 1.79: 491,
#  3.33: 815, 2.11: 353, 2.37: 305, 3.2: 824, 3.01: 737, 1.41: 407, 3.58: 944, 2.69: 398, 3.84: 944, 1.6: 419, 2.18: 278,
#  1.92: 500, 1.02: 329, 2.88: 701, 3.07: 380, 2.94: 725, 3.46: 503, 2.43: 314, 3.97: 806, 1.86: 203, 1.54: 410, 2.5: 347,
#  4.03: 563, 1.98: 200, 3.9: 542, 1.09: 200, 4.16: 1085, 3.26: 485, 1.73: 341, 1.34: 200, 3.39: 446, 3.78: 908,
#  3.65: 563, 4.22: 707, 1.15: 323, 1.66: 428, 0.96: 200, 4.1: 608, 4.61: 848, 4.74: 806, 1.47: 332, 4.42: 1010,
#  3.14: 725, 4.54: 1088, 5.06: 998, 1.22: 362, 4.29: 1031, 4.48: 746, 0.9: 200, 4.35: 749, 4.99: 758, 5.57: 854,
#  5.18: 578, 4.93: 665, 4.67: 584, 4.8: 782, 3.1: 608, 2.98: 602, 3.27: 467, 2.83: 605, 3.73: 752, 3.7: 866, 3.89: 1016,
#  3.54: 536, 3.13: 470, 4.72: 920, 3.43: 692, 3.66: 902, 3.19: 506, 3.03: 767, 3.06: 419, 3.12: 845, 2.8: 617, 3.4: 947,
#  3.51: 896, 2.76: 641, 4.27: 929, 3.25: 902, 3.34: 839, 3.57: 641, 2.96: 626, 3.69: 884, 2.78: 461, 3.09: 758,
#  2.93: 692, 4.53: 608, 4.14: 791, 4.08: 1091, 3.35: 872, 2.99: 617, 4.41: 857, 3.17: 494, 2.79: 710, 3.94: 935,
#  4.24: 1019, 3.24: 707, 3.15: 563, 4.76: 806, 3.79: 725, 3.41: 788, 4.18: 1028, 3.6: 596, 4.97: 875, 3.45: 914,
#  3.16: 542, 4.38: 695, 2.9: 716, 4.39: 422, 5.21: 1079, 3.18: 464, 3.95: 929, 3.08: 470, 3.11: 617, 3.72: 983,
#  3.77: 965, 3.02: 677, 4.05: 1046, 3.21: 683, 2.85: 596, 3.49: 911, 3.05: 488, 3.87: 746, 3.88: 668, 3.63: 914,
#  5.1: 824, 5.02: 839, 3.74: 566, 3.62: 905, 2.77: 524, 4.11: 713, 4.55: 713, 2.91: 611, 4.09: 1088, 2.87: 437,
#  5.77: 917, 3.29: 479, 5.2: 965, 4.95: 959, 4.94: 914, 3.8: 653, 3.96: 1034, 3.82: 863, 3.3: 419, 3.99: 689, 4.01: 1067,
#  3.68: 668, 4.34: 1082, 5.32: 893, 6.61: 926, 2.81: 374, 2.84: 599, 3.5: 734, 3.04: 515, 2.89: 794, 4.6: 1097,
#  2.97: 644, 4.3: 827, 4.25: 644, 4.19: 956, 4.0: 815, 3.86: 689, 4.23: 1097, 3.22: 674, 3.32: 887, 6.12: 1049,
#  5.15: 1028, 3.67: 467, 4.31: 941, 3.83: 986, 3.37: 617, 3.44: 533, 2.92: 620, 4.26: 983, 4.28: 938, 4.45: 1031,
#  3.76: 911, 4.77: 1070, 4.13: 866, 4.06: 1055, 3.47: 776, 4.57: 749, 3.28: 530, 4.85: 860, 3.36: 644, 5.05: 890,
#  2.95: 839, 5.68: 965, 3.53: 734, 5.49: 983, 4.64: 962, 3.31: 563, 3.93: 803, 3.42: 776, 5.17: 935, 4.59: 986,
#  4.32: 1079, 3.75: 569, 4.78: 992, 3.64: 701, 4.21: 1022, 4.44: 908, 3.56: 767, 4.2: 854, 5.09: 977, 3.55: 665,
#  3.59: 668, 4.5: 854, 3.91: 770, 4.37: 647, 3.81: 1016, 4.15: 962, 3.98: 779, 4.17: 767, 3.48: 653, 5.01: 941,
#  4.47: 749, 5.07: 782, 4.51: 1019, 5.08: 1058, 4.63: 989, 4.07: 614, 3.0: 488, 4.66: 920, 5.13: 1055, 5.92: 686,
#  4.49: 902, 5.67: 926, 5.24: 929, 4.04: 623, 5.14: 869, 5.33: 968, 4.75: 926, 4.33: 812, 3.85: 797, 4.02: 524,
#  3.38: 584, 2.86: 458, 5.11: 872, 4.36: 734, 5.04: 692, 5.28: 872, 3.23: 656, 4.4: 725, 5.94: 1016, 5.12: 746,
#  4.86: 824, 6.08: 1088, 4.88: 644, 3.61: 950, 5.44: 845, 5.82: 857, 5.63: 704, 5.5: 1085, 5.89: 962, 2.09: 533,
#  2.73: 407, 2.41: 452, 5.31: 797, 1.81: 449, 5.7: 905, 1.51: 200, 1.49: 215, 2.71: 620, 6.4: 764, 4.65: 1043, 2.26: 584,
#  6.59: 1019, 5.38: 881, 5.76: 1040, 6.78: 752, 0.77: 239, 7.04: 971, 0.7: 215, 5.25: 776, 6.46: 872, 0.51: 200,
#  0.64: 218, 5.95: 938, 6.02: 956, 6.14: 968, 6.27: 944, 6.66: 962, 6.53: 752, 6.85: 1088, 6.21: 593, 7.49: 1094,
#  7.55: 761, 6.34: 1022, 1.88: 302, 2.72: 701, 2.02: 389, 2.23: 431, 2.65: 461, 1.74: 425, 2.16: 350, 1.67: 401,
#  2.58: 620, 1.95: 461, 1.53: 386, 2.51: 473, 4.81: 686, 6.9: 1091, 2.44: 266, 1.46: 221, 5.43: 914, 5.36: 722,
#  4.46: 716, 8.45: 923, 1.39: 356, 6.62: 1094, 5.64: 860, 5.22: 887, 8.19: 1085, 8.57: 1052, 1.32: 242, 6.97: 992,
#  5.78: 599, 1.25: 200, 7.11: 959, 7.45: 968, 6.91: 968, 7.31: 803, 5.99: 674, 8.08: 926, 6.06: 899, 6.76: 1061,
#  6.72: 878}

# pretrained_data_dict = {3.52: 68, 2.24: 30, 2.56: 18, 3.26: 58, 2.75: 46, 2.43: 24, 2.05: 44, 1.15: 18, 1.79: 30,
#                         2.62: 36, 2.69: 46, 3.39: 64, 3.01: 40, 1.41: 16, 3.58: 74, 2.37: 34, 1.54: 22, 3.84: 72,
#                         1.6: 28, 2.82: 40, 2.11: 36, 2.18: 30, 1.73: 20, 1.86: 50, 1.02: 10, 3.2: 44, 1.98: 12,
#                         2.88: 40, 3.07: 46, 1.92: 28, 2.94: 40, 3.46: 54, 2.5: 16, 1.22: 20, 4.1: 54, 3.33: 66,
#                         3.71: 56, 3.9: 78, 1.09: 18, 4.16: 78, 1.34: 24, 1.28: 30, 3.97: 58, 3.14: 54, 2.3: 22,
#                         4.22: 76, 1.66: 46, 0.77: 36, 4.03: 72, 5.18: 76, 3.65: 64, 4.74: 64, 1.47: 22, 3.78: 66,
#                         4.42: 80, 4.48: 68, 4.67: 90, 4.99: 86, 4.35: 52, 4.54: 80, 6.4: 80, 4.86: 90, 5.12: 78,
#                         4.29: 72, 4.93: 84, 4.61: 86, 5.38: 70, 5.5: 84, 0.96: 14, 0.9: 30, 5.06: 74, 4.8: 80, 5.44: 84,
#                         5.89: 66, 5.7: 64, 7.55: 68, 6.53: 42, 0.51: 12, 3.1: 44, 2.98: 34, 3.27: 40, 2.83: 46,
#                         3.73: 50, 3.7: 76, 3.89: 46, 3.54: 52, 3.13: 30, 4.72: 90, 3.43: 74, 3.66: 42, 3.19: 46,
#                         3.03: 62, 3.06: 40, 3.12: 40, 2.8: 34, 3.4: 68, 3.51: 44, 2.76: 34, 4.27: 54, 3.25: 52,
#                         3.34: 60, 3.57: 56, 2.96: 26, 3.69: 56, 2.78: 48, 3.09: 68, 2.93: 30, 4.53: 78, 4.14: 38,
#                         4.08: 64, 3.35: 36, 2.99: 30, 5.2: 74, 3.17: 50, 2.79: 58, 3.94: 52, 4.24: 56, 3.24: 42,
#                         3.15: 44, 4.76: 58, 3.79: 38, 3.41: 48, 4.18: 40, 3.6: 62, 3.45: 54, 3.16: 66, 4.38: 88,
#                         4.95: 68, 2.9: 34, 3.87: 72, 5.21: 68, 5.84: 80, 3.18: 70, 3.95: 76, 3.08: 36, 3.11: 38,
#                         3.72: 40, 3.77: 52, 3.02: 32, 4.05: 68, 3.21: 32, 2.85: 42, 3.49: 52, 3.05: 70, 3.88: 68,
#                         3.63: 36, 5.1: 88, 5.02: 84, 3.74: 34, 3.62: 42, 2.77: 44, 4.11: 48, 4.55: 84, 2.91: 30,
#                         4.82: 68, 4.09: 44, 2.87: 42, 5.77: 88, 3.29: 42, 4.94: 78, 3.8: 42, 3.96: 76, 3.3: 42,
#                         6.06: 78, 3.99: 32, 4.01: 70, 3.68: 56, 4.34: 86, 5.32: 56, 2.81: 38, 2.84: 32, 3.5: 72,
#                         3.04: 38, 2.89: 40, 2.97: 44, 4.3: 64, 5.3: 86, 4.25: 84, 4.0: 72, 3.86: 54, 4.23: 20, 4.78: 46,
#                         3.22: 40, 3.32: 66, 6.12: 88, 4.62: 52, 3.67: 64, 4.31: 76, 3.83: 62, 3.37: 52, 5.72: 84,
#                         3.44: 48, 2.92: 62, 4.26: 88, 4.19: 36, 5.96: 70, 4.28: 68, 4.45: 78, 3.76: 40, 4.49: 46,
#                         4.77: 36, 6.51: 58, 4.13: 58, 4.06: 40, 3.47: 50, 5.53: 62, 4.57: 64, 4.33: 70, 3.28: 58,
#                         4.69: 76, 4.89: 64, 4.85: 78, 3.36: 58, 2.95: 64, 5.86: 82, 7.24: 58, 4.9: 58, 3.53: 26,
#                         3.82: 26, 6.34: 62, 5.49: 54, 6.36: 68, 4.64: 72, 5.15: 64, 3.31: 64, 3.93: 56, 4.5: 54,
#                         3.42: 46, 5.17: 66, 4.59: 64, 4.32: 50, 4.21: 62, 4.71: 56, 3.75: 70, 5.37: 86, 6.74: 76,
#                         3.64: 62, 5.52: 76, 4.44: 50, 4.88: 64, 7.14: 80, 5.04: 74, 3.56: 44, 4.2: 66, 6.88: 76,
#                         5.09: 86, 6.16: 80, 3.55: 66, 3.59: 54, 5.78: 56, 4.43: 50, 5.16: 40, 3.91: 42, 4.37: 72,
#                         3.81: 48, 4.15: 40, 3.98: 34, 4.46: 64, 6.52: 70, 5.87: 66, 6.46: 72, 4.17: 62, 5.23: 60,
#                         3.48: 42, 4.66: 60, 5.01: 60, 5.33: 86, 4.47: 66, 5.07: 70, 4.73: 54, 5.83: 60, 4.51: 32,
#                         5.08: 80, 5.41: 72, 4.63: 64, 7.2: 46, 6.45: 78, 4.07: 54, 3.0: 42, 5.13: 50, 5.92: 78,
#                         5.67: 80, 5.24: 66, 4.04: 48, 5.14: 90, 5.68: 76, 4.41: 34, 5.81: 68, 5.36: 58, 6.89: 46,
#                         4.75: 76, 4.81: 52, 3.85: 32, 3.38: 44, 2.86: 42, 5.11: 82, 4.36: 66, 6.2: 80, 7.07: 80,
#                         11.29: 64, 5.28: 60, 3.23: 32, 5.05: 48, 8.77: 56, 7.54: 68, 4.4: 48, 5.94: 54, 7.4: 60,
#                         11.33: 84, 6.08: 74, 3.61: 62, 5.95: 56, 5.82: 80, 2.09: 36, 2.73: 50, 2.41: 28, 1.81: 32,
#                         1.51: 18, 1.49: 20, 2.71: 56, 5.48: 58, 5.25: 90, 4.65: 58, 2.26: 28, 6.14: 66, 6.85: 88,
#                         6.78: 90, 6.91: 80, 7.68: 78, 6.72: 84, 10.56: 72, 7.04: 82, 6.98: 66, 0.7: 28, 5.63: 74,
#                         5.31: 88, 6.21: 74, 11.01: 80, 6.02: 88, 0.64: 12, 7.1: 90, 7.42: 84, 7.17: 84, 6.27: 74,
#                         6.66: 44, 5.76: 72, 7.23: 42, 5.57: 62, 6.59: 66, 8.25: 84, 8.13: 68, 8.64: 56, 7.3: 78,
#                         7.36: 74, 9.02: 78, 9.34: 76, 8.0: 78, 7.61: 78, 8.06: 74, 8.51: 56, 7.93: 84}


# slurp_data_dict = {2.11: 72, 2.82: 56, 3.71: 90, 2.24: 60, 2.3: 70, 2.56: 84, 2.75: 74, 1.28: 46, 2.43: 78, 2.05: 70,
#                    1.79: 54, 2.62: 64, 3.33: 90, 2.37: 48, 2.94: 78, 3.2: 90, 1.41: 44, 1.54: 74, 3.84: 82, 1.6: 50,
#                    2.18: 90, 1.73: 60, 1.86: 54, 0.9: 20, 2.69: 90, 1.98: 64, 3.14: 70, 1.92: 60, 3.46: 52, 1.34: 42,
#                    2.5: 84, 4.1: 72, 3.97: 68, 1.09: 14, 3.26: 74, 3.39: 68, 3.9: 80, 2.88: 70, 1.02: 42, 3.01: 54,
#                    1.15: 44, 3.07: 44, 1.66: 56, 0.96: 32, 3.52: 78, 4.22: 72, 1.47: 64, 3.65: 66, 3.78: 72, 3.58: 74,
#                    4.74: 84, 6.4: 70, 1.22: 40, 4.35: 86, 4.42: 36, 4.48: 82, 4.16: 90, 4.03: 88, 5.18: 74, 5.06: 58,
#                    4.29: 78, 3.73: 84, 3.13: 62, 3.43: 42, 3.19: 68, 3.03: 52, 3.12: 64, 2.8: 86, 2.76: 58, 3.25: 68,
#                    3.34: 64, 3.89: 66, 3.57: 56, 2.96: 90, 3.69: 52, 2.78: 38, 3.09: 42, 2.93: 68, 3.17: 44, 2.79: 38,
#                    3.35: 76, 3.94: 44, 3.79: 42, 3.41: 74, 3.6: 38, 2.9: 44, 3.18: 38, 3.08: 34, 3.77: 66, 3.02: 38,
#                    3.21: 58, 2.85: 72, 3.27: 74, 3.88: 72, 3.63: 46, 3.11: 88, 3.74: 42, 3.62: 72, 2.77: 86, 2.91: 80,
#                    3.1: 46, 2.87: 90, 3.4: 40, 2.98: 88, 3.66: 88, 3.68: 42, 3.51: 64, 2.83: 78, 3.5: 50, 3.05: 90,
#                    3.06: 74, 3.32: 90, 2.92: 88, 3.16: 60, 4.28: 62, 4.45: 56, 3.76: 76, 4.49: 78, 4.77: 44, 4.31: 80,
#                    4.06: 82, 3.47: 64, 5.53: 90, 4.57: 64, 4.05: 62, 4.33: 72, 3.28: 48, 4.69: 80, 4.89: 54, 3.83: 64,
#                    4.85: 52, 3.36: 40, 4.09: 48, 2.95: 36, 3.04: 86, 2.97: 60, 4.95: 86, 4.9: 80, 4.25: 50, 5.68: 58,
#                    3.53: 52, 4.11: 56, 4.19: 80, 3.82: 32, 4.18: 76, 4.14: 46, 5.49: 72, 6.36: 64, 4.27: 76, 5.84: 90,
#                    3.8: 64, 3.22: 40, 2.84: 90, 4.01: 40, 3.31: 66, 3.93: 44, 3.49: 36, 3.29: 56, 4.5: 66, 3.42: 44,
#                    3.24: 42, 4.38: 46, 5.32: 66, 5.17: 80, 4.59: 54, 4.32: 56, 4.21: 46, 4.78: 68, 2.99: 84, 3.75: 46,
#                    3.64: 86, 2.89: 48, 3.7: 48, 3.99: 50, 4.61: 56, 3.3: 52, 4.53: 76, 3.86: 64, 3.96: 60, 4.44: 50,
#                    4.88: 40, 3.56: 52, 4.2: 46, 3.55: 56, 3.59: 44, 5.78: 82, 4.26: 32, 4.43: 52, 5.16: 62, 3.91: 50,
#                    4.3: 68, 4.37: 44, 3.67: 34, 5.63: 84, 3.81: 44, 4.15: 60, 3.98: 50, 4.46: 36, 3.37: 56, 4.17: 60,
#                    5.23: 56, 3.48: 58, 4.66: 50, 5.01: 60, 4.23: 52, 4.47: 58, 5.07: 72, 4.73: 74, 5.83: 44, 3.54: 54,
#                    4.51: 54, 5.41: 78, 3.72: 48, 3.45: 88, 4.71: 68, 4.63: 74, 4.07: 50, 3.15: 82, 2.86: 82, 2.09: 48,
#                    2.73: 54, 2.41: 58, 1.81: 44, 1.51: 32, 1.49: 32, 2.71: 80, 2.26: 62, 0.77: 48, 4.67: 76, 0.7: 42,
#                    0.51: 34, 0.64: 20, 5.25: 62, 4.99: 70, 4.54: 80, 5.7: 88, 4.8: 78, 5.12: 66, 4.86: 64, 4.93: 76}

# aug 20
slurp_L2 = {2.11: 48, 2.82: 82, 3.71: 72, 2.24: 48, 2.3: 80, 2.56: 54, 2.75: 64, 1.28: 32, 2.43: 76, 2.05: 76, 1.79: 68,
            2.62: 66, 3.33: 90, 2.37: 86, 2.94: 68, 3.2: 88, 1.41: 38, 1.54: 34, 3.84: 70, 1.6: 64, 2.18: 74, 1.73: 44,
            1.86: 50, 0.9: 26, 2.69: 62, 1.98: 64, 3.14: 86, 1.92: 68, 3.46: 78, 1.34: 52, 2.5: 82, 3.97: 78, 1.09: 12,
            3.26: 60, 3.39: 68, 3.9: 82, 3.52: 80, 2.88: 90, 1.02: 42, 3.01: 68, 1.15: 10, 1.66: 64, 0.96: 18, 3.65: 78,
            4.22: 86, 1.47: 46, 3.78: 70, 3.58: 76, 3.07: 90, 4.74: 76, 6.4: 66, 1.22: 22, 4.35: 70, 4.42: 76, 4.48: 74,
            4.16: 74, 4.1: 84, 4.03: 74, 5.18: 80, 5.06: 62, 4.29: 84, 3.73: 80, 3.7: 48, 3.13: 80, 3.43: 36, 3.19: 64,
            3.03: 52, 3.12: 66, 2.8: 80, 2.76: 60, 3.25: 62, 3.34: 86, 3.89: 62, 3.57: 58, 2.96: 90, 3.69: 86, 2.78: 32,
            3.09: 44, 2.93: 64, 3.17: 44, 2.79: 84, 3.35: 76, 3.94: 44, 3.79: 44, 3.41: 66, 3.6: 30, 4.38: 46, 2.9: 90,
            3.18: 36, 3.08: 36, 3.77: 68, 3.02: 88, 3.21: 62, 2.85: 66, 3.27: 88, 3.88: 70, 3.63: 48, 3.11: 62,
            3.74: 38, 3.62: 78, 2.77: 88, 2.91: 78, 3.1: 84, 2.87: 88, 3.4: 40, 2.98: 78, 3.66: 84, 3.68: 38, 3.51: 68,
            2.83: 76, 3.5: 50, 3.05: 90, 3.06: 82, 3.32: 48, 2.92: 84, 3.16: 58, 4.28: 58, 4.45: 52, 3.76: 58, 4.49: 90,
            4.77: 44, 4.31: 78, 4.06: 84, 3.47: 56, 5.53: 72, 4.57: 62, 4.05: 58, 4.33: 66, 3.28: 44, 4.69: 82,
            4.89: 56, 3.83: 86, 4.85: 50, 3.36: 38, 4.09: 48, 2.95: 38, 3.04: 56, 2.97: 64, 4.95: 82, 4.9: 82, 4.25: 76,
            5.68: 66, 3.53: 48, 4.11: 64, 4.19: 84, 3.82: 38, 4.18: 70, 4.14: 52, 5.49: 82, 6.36: 70, 4.27: 86, 3.8: 60,
            3.22: 48, 2.84: 64, 4.01: 38, 3.31: 64, 3.93: 38, 3.49: 38, 3.29: 52, 4.5: 64, 3.42: 42, 4.23: 52, 3.24: 44,
            5.32: 62, 5.17: 80, 4.59: 60, 4.32: 58, 4.21: 40, 4.78: 72, 2.99: 86, 3.75: 54, 3.64: 84, 2.89: 48,
            3.99: 48, 4.61: 66, 3.3: 50, 4.53: 88, 3.86: 60, 3.96: 54, 4.88: 40, 3.56: 50, 4.2: 46, 6.88: 84, 3.55: 74,
            3.59: 40, 5.78: 90, 4.26: 34, 4.44: 48, 4.43: 46, 5.16: 64, 3.91: 52, 4.3: 64, 4.37: 46, 3.67: 36, 5.63: 88,
            3.81: 44, 4.15: 56, 3.98: 46, 4.46: 88, 3.37: 54, 4.17: 60, 5.23: 62, 3.48: 78, 4.66: 54, 5.01: 60,
            4.47: 58, 5.07: 62, 4.73: 82, 5.83: 44, 3.54: 58, 4.51: 52, 5.41: 76, 3.72: 42, 3.45: 68, 4.54: 82,
            4.71: 74, 4.63: 84, 4.07: 54, 3.15: 72, 4.41: 84, 2.86: 64, 2.09: 24, 2.73: 60, 2.41: 56, 1.81: 40,
            1.51: 32, 1.49: 28, 2.71: 80, 2.81: 88, 2.26: 52, 0.77: 36, 4.67: 78, 0.7: 40, 0.51: 36, 0.64: 26, 4.93: 82,
            5.25: 58, 4.99: 90, 5.7: 88, 4.8: 38, 6.27: 90, 5.57: 90, 5.12: 54, 5.89: 90, 5.76: 90, 4.86: 64, 5.31: 82,
            7.36: 80, 1.88: 44, 2.72: 64, 3.0: 88, 2.02: 56, 2.23: 32, 2.65: 42, 1.74: 58, 2.16: 38, 1.67: 36, 2.58: 58,
            1.95: 24, 1.53: 46, 2.51: 44, 2.44: 64, 1.46: 34, 4.81: 54, 5.38: 82, 1.39: 32, 5.92: 78, 1.32: 32,
            1.25: 32, 6.53: 44, 7.24: 44, 4.04: 68, 5.29: 72}
# Convert the dictionary keys and values into tensors
keys = torch.tensor(list(slurp_L2.keys()), dtype=torch.float32).unsqueeze(1)
values = torch.tensor(list(slurp_L2.values()), dtype=torch.float32).unsqueeze(1)

# Define the hyperparameters
input_size = 1  # Since we have single-dimensional keys
hidden_size = 64
output_size = 1  # Predicting single-dimensional values
learning_rate = 0.001  # Adjust the learning rate
batch_size = 16
num_epochs = 200

# Create a DataLoader for the data
dataset = TensorDataset(keys, values)
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

# Create the MLP model
model = MLP(input_size, hidden_size, output_size)

# Define the loss function and optimizer
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Training loop
for epoch in range(num_epochs):
    running_loss = 0.0
    for inputs, labels in dataloader:
        optimizer.zero_grad()

        # Forward pass
        outputs = model(inputs)
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    if (epoch + 1) % 10 == 0:
        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss:.4f}')

saved_data = {
    'model': model
}

filename = f'slurp-MLP-L2.pkl'
with open(filename, 'wb') as f:
    pickle.dump(saved_data, f)

filename = f'slurp-MLP-L2.pkl'
with open(filename, 'rb') as f:
    load_data = pickle.load(f)
model = load_data['model']

x_values = [x / 2 for x in range(2, 21)]
y_values = []
for test_key in x_values:
    test_input = torch.tensor([[test_key]], dtype=torch.float32)
    # macs, params = profile(model, inputs=test_input)
    predicted_value = model(test_input)
    predicted_value = predicted_value.item()
    y_values.append(predicted_value)
    print(f'For key {test_key:.2f}, predicted value is {predicted_value:.2f}')

plt.xlabel('Duration')
plt.ylabel('Threshold')

# Plot the data
plt.plot(x_values, y_values, label='threshold')

plt.legend()
# plt.savefig('optimal_threshold.png')
# Show the plot
plt.show()
print()
